{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"cardio_base.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>ap_hi</th>\n",
       "      <th>ap_lo</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>smoke</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>18393</td>\n",
       "      <td>2</td>\n",
       "      <td>168</td>\n",
       "      <td>62.0</td>\n",
       "      <td>110</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>20228</td>\n",
       "      <td>1</td>\n",
       "      <td>156</td>\n",
       "      <td>85.0</td>\n",
       "      <td>140</td>\n",
       "      <td>90</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>18857</td>\n",
       "      <td>1</td>\n",
       "      <td>165</td>\n",
       "      <td>64.0</td>\n",
       "      <td>130</td>\n",
       "      <td>70</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>17623</td>\n",
       "      <td>2</td>\n",
       "      <td>169</td>\n",
       "      <td>82.0</td>\n",
       "      <td>150</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>17474</td>\n",
       "      <td>1</td>\n",
       "      <td>156</td>\n",
       "      <td>56.0</td>\n",
       "      <td>100</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id    age  gender  height  weight  ap_hi  ap_lo  cholesterol  smoke\n",
       "0   0  18393       2     168    62.0    110     80            1      0\n",
       "1   1  20228       1     156    85.0    140     90            3      0\n",
       "2   2  18857       1     165    64.0    130     70            3      0\n",
       "3   3  17623       2     169    82.0    150    100            1      0\n",
       "4   4  17474       1     156    56.0    100     60            1      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(gender         0\n",
       " height         0\n",
       " weight         0\n",
       " ap_hi          0\n",
       " ap_lo          0\n",
       " cholesterol    0\n",
       " smoke          0\n",
       " age_years      0\n",
       " dtype: int64,\n",
       "    gender    height    weight     ap_hi     ap_lo  smoke  age_years  \\\n",
       " 0     1.0  0.579487  0.273684  0.016079  0.013550    0.0   0.588076   \n",
       " 1     0.0  0.517949  0.394737  0.017934  0.014453    0.0   0.730159   \n",
       " 2     0.0  0.564103  0.284211  0.017316  0.012647    0.0   0.624003   \n",
       " 3     1.0  0.584615  0.378947  0.018553  0.015357    0.0   0.528455   \n",
       " 4     0.0  0.517949  0.242105  0.015461  0.011743    0.0   0.516918   \n",
       " \n",
       "    cholesterol  \n",
       " 0            1  \n",
       " 1            3  \n",
       " 2            3  \n",
       " 3            1  \n",
       " 4            1  )"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert age from days to years\n",
    "data['age_years'] = data['age'] / 365.25\n",
    "\n",
    "# Drop the original 'age' column and 'id' as they are not needed for modeling\n",
    "data.drop(['age', 'id'], axis=1, inplace=True)\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = data.isnull().sum()\n",
    "\n",
    "# Normalize the data (excluding 'cholesterol' as it's our target variable)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "features = data.drop('cholesterol', axis=1)\n",
    "scaler = MinMaxScaler()\n",
    "features_scaled = pd.DataFrame(scaler.fit_transform(features), columns=features.columns)\n",
    "\n",
    "# Prepare the final dataset for KNN\n",
    "final_data = features_scaled\n",
    "final_data['cholesterol'] = data['cholesterol']\n",
    "\n",
    "missing_values, final_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7214285714285714"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X = final_data.drop('cholesterol', axis=1)\n",
    "y = final_data['cholesterol']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a KNN model\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict cholesterol levels on the test set\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_reports = classification_report(y_test, y_pred,output_dict=True)\n",
    "\n",
    "accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+----------+------------+-----------+\n",
      "|              |   precision |   recall |   f1-score |   support |\n",
      "|--------------+-------------+----------+------------+-----------|\n",
      "| 1            |        0.75 |     0.95 |       0.84 |  10475.00 |\n",
      "| 2            |        0.15 |     0.04 |       0.06 |   1881.00 |\n",
      "| 3            |        0.22 |     0.04 |       0.07 |   1644.00 |\n",
      "| accuracy     |        0.72 |     0.72 |       0.72 |      0.72 |\n",
      "| macro avg    |        0.37 |     0.34 |       0.32 |  14000.00 |\n",
      "| weighted avg |        0.61 |     0.72 |       0.65 |  14000.00 |\n",
      "+--------------+-------------+----------+------------+-----------+\n"
     ]
    }
   ],
   "source": [
    "# ! pip install tabulate\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Convert the classification report dictionary to a DataFrame\n",
    "report_df = pd.DataFrame(classification_reports).transpose()\n",
    "\n",
    "# Print the formatted classification report\n",
    "print(tabulate(report_df, headers='keys', tablefmt='psql', showindex=True, floatfmt=\".2f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install imbalanced-learn\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Assuming X and y are your features and target variable\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4599285714285714\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.79      0.51      0.62     10475\n",
      "           2       0.16      0.31      0.21      1881\n",
      "           3       0.15      0.32      0.21      1644\n",
      "\n",
      "    accuracy                           0.46     14000\n",
      "   macro avg       0.36      0.38      0.34     14000\n",
      "weighted avg       0.63      0.46      0.51     14000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Make predictions and evaluate the model\n",
    "y_pred = knn.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cholesterol\n",
       "1    41910\n",
       "2     7668\n",
       "3     6422\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cholesterol\n",
       "2    0.357143\n",
       "3    0.357143\n",
       "1    0.285714\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# Separate the majority and minority classes\n",
    "df_majority = data[data.cholesterol==1]\n",
    "df_minority_2 = data[data.cholesterol==2]\n",
    "df_minority_3 = data[data.cholesterol==3]\n",
    "\n",
    "# Upsample minority classes\n",
    "df_minority_upsampled_2 = resample(df_minority_2, \n",
    "                                   replace=True,     # sample with replacement\n",
    "                                   n_samples=df_majority.shape[0],    # to match majority class\n",
    "                                   random_state=123) # reproducible results\n",
    "\n",
    "df_minority_upsampled_3 = resample(df_minority_3, \n",
    "                                   replace=True,     # sample with replacement\n",
    "                                   n_samples=df_majority.shape[0],    # to match majority class\n",
    "                                   random_state=123) # reproducible results\n",
    "\n",
    "# Combine majority class with upsampled minority classes\n",
    "df_upsampled = pd.concat([df_majority, df_minority_upsampled_2, df_minority_upsampled_3])\n",
    "\n",
    "# Undersample the majority class slightly to avoid a sudden jump in class sizes\n",
    "df_majority_downsampled = resample(df_majority, \n",
    "                                   replace=False,    # sample without replacement\n",
    "                                   n_samples=int(df_majority.shape[0]*0.8),     # to decrease majority class size\n",
    "                                   random_state=123) # reproducible results\n",
    "\n",
    "# Combine the downsampled majority class with upsampled minority classes again\n",
    "df_balanced = pd.concat([df_majority_downsampled, df_minority_upsampled_2, df_minority_upsampled_3])\n",
    "\n",
    "# New class distribution\n",
    "balanced_class_distribution = df_balanced.cholesterol.value_counts(normalize=True)\n",
    "\n",
    "# Prepare the features and target variable for the balanced dataset\n",
    "X_balanced = df_balanced.drop('cholesterol', axis=1)\n",
    "y_balanced = df_balanced['cholesterol']\n",
    "\n",
    "# Split the balanced data into new training and testing sets\n",
    "X_train_balanced, X_test_balanced, y_train_balanced, y_test_balanced = train_test_split(\n",
    "    X_balanced, y_balanced, test_size=0.2, random_state=42)\n",
    "\n",
    "balanced_class_distribution\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7727365694027816"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrain the KNN model with the balanced dataset\n",
    "knn_balanced = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_balanced.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Predict cholesterol levels on the balanced test set\n",
    "y_pred_balanced = knn_balanced.predict(X_test_balanced)\n",
    "\n",
    "# Evaluate the retrained model\n",
    "accuracy_balanced = accuracy_score(y_test_balanced, y_pred_balanced)\n",
    "classification_report_balanced = classification_report(y_test_balanced, y_pred_balanced,output_dict=True)\n",
    "\n",
    "accuracy_balanced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+----------+------------+-----------+\n",
      "|              |   precision |   recall |   f1-score |   support |\n",
      "|--------------+-------------+----------+------------+-----------|\n",
      "| 1            |        0.78 |     0.41 |       0.54 |   8405.00 |\n",
      "| 2            |        0.75 |     0.90 |       0.82 |  10413.00 |\n",
      "| 3            |        0.79 |     0.93 |       0.86 |  10518.00 |\n",
      "| accuracy     |        0.77 |     0.77 |       0.77 |      0.77 |\n",
      "| macro avg    |        0.78 |     0.75 |       0.74 |  29336.00 |\n",
      "| weighted avg |        0.77 |     0.77 |       0.75 |  29336.00 |\n",
      "+--------------+-------------+----------+------------+-----------+\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "# Convert the classification report dictionary to a DataFrame\n",
    "report_df_balanced = pd.DataFrame(classification_report_balanced).transpose()\n",
    "\n",
    "# Print the formatted classification report\n",
    "print(tabulate(report_df_balanced, headers='keys', tablefmt='psql', showindex=True, floatfmt=\".2f\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instance-based learning**, specifically the k-Nearest Neighbors (KNN) algorithm, is known as a \"lazy learning\" approach because it does not build a model or learn a function from the training data during the training phase. Instead, it stores the entire training dataset and performs the learning or prediction process when a new instance needs to be classified or predicted.\n",
    "\n",
    "The key aspects of instance-based learning and KNN can be summarized as follows:\n",
    "\n",
    "* **No explicit model building**: During the training phase, the algorithm simply stores the training examples (feature vectors and their corresponding labels or target values) in memory. No model or function is learned from the data at this stage.\n",
    "\n",
    "* **Similarity-based prediction**: When a new instance needs to be classified or predicted, the algorithm finds the k most similar instances from the training data to the new instance. Similarity is typically measured using distance metrics like Euclidean distance, Manhattan distance, or cosine similarity, depending on the nature of the data.\n",
    "\n",
    "**d(P, Q) = √[Σ(pi - qi)^2]**\n",
    "\n",
    "\n",
    "* **Nearest neighbor voting/averaging**: For classification tasks, the algorithm assigns the class label based on the majority vote among the k nearest neighbors. For regression tasks, the predicted value is the average or median of the target values of the k nearest neighbors.\n",
    "\n",
    "* **Lazy learning**: The learning process is deferred until a new instance needs to be predicted, making it a \"lazy\" approach. The algorithm does not build a model or learn a function until it encounters a new instance.\n",
    "\n",
    "* **Dimensionality and distance metrics**: The performance of KNN can be affected by the dimensionality of the feature space and the choice of distance metric. As the number of features increases, the distance calculations become less reliable, and the algorithm may need to be adapted or combined with dimensionality reduction techniques.\n",
    "\n",
    "* **Memory and computational complexity**: KNN requires storing the entire training dataset, which can be memory-intensive for large datasets. Additionally, the prediction process involves computing distances between the new instance and all training instances, which can be computationally expensive, especially for large datasets or high-dimensional feature spaces.\n",
    "\n",
    "* **Parameter tuning**: The choice of the value of k (the number of nearest neighbors) and the distance metric can significantly impact the performance of the KNN algorithm. Cross-validation or grid search techniques are often used to select the optimal value of k and the appropriate distance metric for a given dataset.\n",
    "\n",
    "\n",
    "In summary, instance-based learning and KNN are unique in their approach of deferring the learning process until prediction time, relying solely on the similarity between the new instance and the stored training examples. This lazy learning approach has advantages in terms of simplicity and adaptability but also faces challenges related to dimensionality, memory requirements, and computational complexity, which need to be addressed through appropriate techniques and parameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the k-Nearest Neighbors (KNN) algorithm, the choice of k and the weighting of attributes can have a significant impact on the performance of the model. Here are some general guidelines and considerations:\n",
    "\n",
    "\n",
    "**Equal Weighting of Attributes**:\n",
    "* Equal weighting of attributes is appropriate when there is no prior knowledge or reason to believe that some features are more important than others in determining the target variable.\n",
    "* It is a simple and straightforward approach that treats all features equally.\n",
    "* However, if there is prior knowledge or domain expertise that suggests certain features are more relevant or discriminative, it may be better to assign higher weights to those features.\n",
    "\n",
    "**Choice of k**:\n",
    "\n",
    "**Larger k**:\n",
    "\n",
    "* A larger value of k tends to reduce the noise in the classification or regression, making the model more stable and less sensitive to outliers.\n",
    "\n",
    "* However, a larger k may also lead to smoother decision boundaries, potentially missing fine details or local patterns in the data.\n",
    "* A larger k is generally preferred when the data is noisy or when there is a higher risk of overfitting with smaller values of k.\n",
    "\n",
    "**k = 1 (Nearest Neighbor)**:\n",
    "\n",
    "* Setting k = 1 means that the prediction is based on the single nearest neighbor, making the model highly sensitive to individual data points and potential outliers.\n",
    "* k = 1 can lead to overfitting, especially in cases where the training data is noisy or has overlapping class distributions.\n",
    "* However, k = 1 can be useful when the decision boundaries are highly complex or irregular, and a higher k might over-smooth the boundaries.\n",
    "\n",
    "In general, the choice of k and attribute weighting should be guided by the characteristics of the dataset, prior knowledge, and the trade-off between bias and variance. Here are some common practices:\n",
    "\n",
    "* Cross-validation: Use techniques like k-fold cross-validation or leave-one-out cross-validation to evaluate different values of k and attribute weightings. The combination that yields the best performance on the validation set can be chosen for the final model.\n",
    "* Domain knowledge: If there is prior knowledge or domain expertise that suggests certain features are more important, assign higher weights to those features.\n",
    "* Feature scaling: Scale the features to a common range, especially if the features have different units or scales, to prevent some features from dominating the distance calculations.\n",
    "* Feature selection: Perform feature selection techniques to identify and remove redundant or irrelevant features, which can improve the performance of KNN and reduce the curse of dimensionality.\n",
    "\n",
    "It's important to note that KNN is a **non-parametric algorithm**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Weighted Example**\n",
    "\n",
    "One way to assign different weights to features in KNN is by using a weighted distance metric. Instead of using the standard Euclidean distance, which treats all features equally, you can introduce weights to the distance calculation. Here's an example:\n",
    "\n",
    "Suppose you have a dataset with three features: age, income, and credit score, and you want to predict whether a person will default on a loan or not. Based on domain knowledge or prior analysis, you know that credit score is a more important factor in determining loan default risk compared to age or income.\n",
    "\n",
    "You can assign a higher weight to the credit score feature by modifying the distance calculation as follows:\n",
    "\n",
    "weighted_distance = sqrt((weight_age * (age1 - age2)^2) + (weight_income * (income1 - income2)^2) + (weight_credit_score * (credit_score1 - credit_score2)^2))\n",
    "\n",
    "\n",
    "you can set weight_credit_score to a higher value, such as 3, while keeping the weights for age and income at 1. This way, the credit score feature will have a greater influence on the distance calculation and, consequently, on the predictions made by the KNN algorithm.\n",
    "\n",
    "\n",
    "In the k-Nearest Neighbors (KNN) algorithm, the distance between the new data point (whose label or target value needs to be predicted) and the existing data points in the training set is calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cardio_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
